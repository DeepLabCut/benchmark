
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>API Docs &#8212; DeepLabCut benchmark</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="How to submit your models to the benchmark" href="submission.html" /> 
    <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.31.3/js/jquery.tablesorter.min.js" integrity="sha512-qzgd5cYSZcosqpzpn7zF2ZId8f/8CHmFKZ8j7mU4OUXTNRd5g+ZHBPsgKEwoqxCtdQvExE5LprwwPAgoicguNg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xLiPY/NS5R+E6ztJQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script>
      $(function() {
        $("#myTable").tablesorter();
      });
    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/dlc-logo-violet.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="index.html">
  Leaderboard
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="datasets.html">
  Datasets
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="submission.html">
  Submission
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  API Docs
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://deeplabcut.org/">DeepLabCut<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/deeplabcut/benchmark" rel="noopener" target="_blank" title="Github">
            <span><i class="fab fa-github"></i></span>
            <label class="sr-only">Github</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/DeepLabCut" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <div class="col-12 col-md-1 col-xl-2 bd-sidebar no-sidebar"></div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-11 col-xl-8 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="api-docs">
<h1>API Docs<a class="headerlink" href="#api-docs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="high-level-api">
<h2>High Level API<a class="headerlink" href="#high-level-api" title="Permalink to this headline">¶</a></h2>
<p>When implementing your own benchmarks, the most important functions are directly accessible
under the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> package.</p>
<span class="target" id="module-benchmark"></span><dl class="py function">
<dt class="sig sig-object py" id="benchmark.evaluate">
<span class="sig-prename descclassname"><span class="pre">benchmark.</span></span><span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">include_benchmarks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">results</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_error</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'return'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#benchmark.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Run evaluation for all benchmarks and methods.</p>
<p>Note that in order for your custom benchmark to be included during
evaluation, the following conditions need to be met:</p>
<blockquote>
<div><ul class="simple">
<li><p>The benchmark subclassed one of the benchmark definitions in
in <code class="docutils literal notranslate"><span class="pre">benchmark.benchmarks</span></code></p></li>
<li><p>The benchmark is registered by applying the <code class="docutils literal notranslate"><span class="pre">&#64;benchmark.register</span></code>
decorator to the class</p></li>
<li><p>The benchmark was imported. This is done automatically for all
benchmarks that are defined in submodules or subpackages of the
<code class="docutils literal notranslate"><span class="pre">benchmark.submissions</span></code> module. For all other locations, make
sure to manually import the packages <strong>before</strong> calling the
<code class="docutils literal notranslate"><span class="pre">evaluate()</span></code> function.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>include_benchmarks</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Container</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – If <code class="docutils literal notranslate"><span class="pre">None</span></code>, run all benchmarks that were discovered. If a container
is passed, only include methods that were defined on benchmarks with
the specified names. E.g., <code class="docutils literal notranslate"><span class="pre">include_benchmarks</span> <span class="pre">=</span> <span class="pre">[&quot;trimouse&quot;]</span></code> would
only evaluate methods of the trimouse benchmark dataset.</p></li>
<li><p><strong>on_error</strong> – see documentation in <code class="docutils literal notranslate"><span class="pre">benchmark.base.Benchmark.evaluate()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ResultCollection</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A collection of all results, which can be printed or exported to
<code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> or <code class="docutils literal notranslate"><span class="pre">json</span></code> file formats.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="benchmark.register">
<span class="sig-prename descclassname"><span class="pre">benchmark.</span></span><span class="sig-name descname"><span class="pre">register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#benchmark.register" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a benchmark to the list of evaluations to run.</p>
<p>Apply this function as a decorator to a class. Note that the
class needs to be a subclass of the <code class="docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code>
base class.</p>
<p>In most situations, it will be a subclass of one of the pre-defined
benchmarks in <code class="docutils literal notranslate"><span class="pre">benchmark.benchmarks</span></code>.</p>
<dl class="simple">
<dt>Throws:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">ValueError</span></code> if the decorator is applied to a class that is
not a subclass of <code class="docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-benchmark.benchmarks">
<span id="available-benchmark-definitions"></span><h2>Available benchmark definitions<a class="headerlink" href="#module-benchmark.benchmarks" title="Permalink to this headline">¶</a></h2>
<p>The actual benchmark definitions.</p>
<dl class="py class">
<dt class="sig sig-object py" id="benchmark.benchmarks.FishBenchmark">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">benchmark.benchmarks.</span></span><span class="sig-name descname"><span class="pre">FishBenchmark</span></span><a class="headerlink" href="#benchmark.benchmarks.FishBenchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code></p>
<p>Dataset with multiple fish, filmed from top-view</p>
<p>Schools of inland silversides (Menidia beryllina, n=14 individuals per school) were recorded in the Lauder Lab at Harvard University while swimming at 15 speeds (0.5 to 8 BL/s, body length, at 0.5 BL/s intervals) in a flow tank with a total working section of 28 x 28 x 40 cm as described in previous work, at a constant temperature (18±1°C) and salinity (33 ppt), at a Reynolds number of approximately 10,000 (based on BL). Dorsal views of steady swimming across these speeds were recorded by high-speed video cameras (FASTCAM Mini AX50, Photron USA, San Diego, CA, USA) at 60-125 frames per second (feeding videos at 60 fps, swimming alone 125 fps). The dorsal view was recorded above the swim tunnel and a floating Plexiglas panel at the water surface prevented surface ripples from interfering with dorsal view videos. Five keypoints were labeled (tip, gill, peduncle, dorsal fin tip, caudal tip). 100 frames were labeled, making this a real-world sized laboratory dataset.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="benchmark.benchmarks.MarmosetBenchmark">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">benchmark.benchmarks.</span></span><span class="sig-name descname"><span class="pre">MarmosetBenchmark</span></span><a class="headerlink" href="#benchmark.benchmarks.MarmosetBenchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code></p>
<p>Dataset with two marmosets.</p>
<p>All animal procedures are overseen by veterinary staff of the MIT and Broad Institute Department of Comparative Medicine, in compliance with the NIH guide for the care and use of laboratory animals and approved by the MIT and Broad Institute animal care and use committees. Video of common marmosets (Callithrix jacchus) was collected in the laboratory of Guoping Feng at MIT. Marmosets were recorded using Kinect V2 cameras (Microsoft) with a resolution of 1080p and frame rate of 30 Hz. After acquisition, images to be used for training the network were manually cropped to 1000 x 1000 pixels or smaller. The dataset is 7,600 labeled frames from 40 different marmosets collected from 3 different colonies (in different facilities). Each cage contains a pair of marmosets, where one marmoset had light blue dye applied to its tufts. One human annotator labeled the 15 marker points on each animal present in the frame (frames contained either 1 or 2 animals).</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="benchmark.benchmarks.ParentingMouseBenchmark">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">benchmark.benchmarks.</span></span><span class="sig-name descname"><span class="pre">ParentingMouseBenchmark</span></span><a class="headerlink" href="#benchmark.benchmarks.ParentingMouseBenchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code></p>
<p>Datasets with three mice, one parenting, two pups.</p>
<p>Parenting behavior is a pup directed behavior observed in adult mice involving complex motor actions directed towards the benefit of the offspring. These experiments were carried out in the laboratory of Catherine Dulac at Harvard University. The behavioral assay was performed in the homecage of singly housed adult female mice in dark/red light conditions. For these videos, the adult mice was monitored for several minutes in the cage followed by the introduction of pup (4 days old) in one corner of the cage. The behavior of the adult and pup was monitored for a duration of 15 minutes. Video was recorded at 30Hz using a Microsoft LifeCam camera (Part#: 6CH-00001) with a resolution of 1280 x 720 pixels or a Geovision camera (model no.: GV-BX4700-3V) also acquired at 30 frames per second at a resolution of 704 x 480 pixels. A human annotator labeled on the adult animal the same 12 body points as in the tri-mouse dataset, and five body points on the pup along its spine. Initially only the two ends were labeled, and intermediate points were added by interpolation and their positions was manually adjusted if necessary. All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee. 542 frames were labeled, making this a real-world sized laboratory dataset.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="benchmark.benchmarks.TriMouseBenchmark">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">benchmark.benchmarks.</span></span><span class="sig-name descname"><span class="pre">TriMouseBenchmark</span></span><a class="headerlink" href="#benchmark.benchmarks.TriMouseBenchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">benchmark.base.Benchmark</span></code></p>
<p>Datasets with three mice with a top-view camera.</p>
<p>Three wild-type (C57BL/6J) male mice ran on a paper spool following odor trails (Mathis et al 2018). These experiments were carried out in the laboratory of Venkatesh N. Murthy at Harvard University. Data were recorded at 30 Hz with 640 x 480 pixels resolution acquired with a Point Grey Firefly FMVU-03MTM-CS. One human annotator was instructed to localize the 12 keypoints (snout, left ear, right ear, shoulder, four spine points, tail base and three tail points). All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee. 161 frames were labeled, making this a real-world sized laboratory dataset.</p>
</dd></dl>

</div>
<div class="section" id="metric-calculation">
<h2>Metric calculation<a class="headerlink" href="#metric-calculation" title="Permalink to this headline">¶</a></h2>
<p>The interface to <code class="docutils literal notranslate"><span class="pre">DeepLabCut</span></code> is implemented in the <code class="docutils literal notranslate"><span class="pre">benchmark.metrics</span></code> package.
For displaying results and testing your submission, it is not required to have DeepLabCut installed.
However, once you intend to (re-) evaluate, make sure to have a working DeepLabCut installation.</p>
<span class="target" id="module-benchmark.metrics"></span><dl class="py function">
<dt class="sig sig-object py" id="benchmark.metrics.calc_map_from_obj">
<span class="sig-prename descclassname"><span class="pre">benchmark.metrics.</span></span><span class="sig-name descname"><span class="pre">calc_map_from_obj</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_results_obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h5_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oks_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric_kpts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_kpts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#benchmark.metrics.calc_map_from_obj" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate mean average precision (mAP) based on predictions.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="benchmark.metrics.calc_rmse_from_obj">
<span class="sig-prename descclassname"><span class="pre">benchmark.metrics.</span></span><span class="sig-name descname"><span class="pre">calc_rmse_from_obj</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_results_obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h5_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_kpts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#benchmark.metrics.calc_rmse_from_obj" title="Permalink to this definition">¶</a></dt>
<dd><p>Calc prediction errors for submissions.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="benchmark.metrics.conv_obj_to_assemblies">
<span class="sig-prename descclassname"><span class="pre">benchmark.metrics.</span></span><span class="sig-name descname"><span class="pre">conv_obj_to_assemblies</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_results_obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoint_names</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#benchmark.metrics.conv_obj_to_assemblies" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert predictions to deeplabcut assemblies.</p>
</dd></dl>

</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, DeepLabCut Developers.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>